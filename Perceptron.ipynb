{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUDoNVX_gdT1"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboXSe0TgdT4"
      },
      "source": [
        "![Image](./basic_perceptron_image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwVbz8IegdT4"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJejv3jegdT5"
      },
      "source": [
        "A perceptron is a single unit of a neural network. You may have seen many Deep Learning models using Neural Networks, all of those models are just a collection of perceptrons.\n",
        "\n",
        "Perceptrons take input data, perform some calculations on them, and output the results. The input data is a collection of numbers, and each input has a weight associated with it. The perceptron multiplies each input number by its weight, sums up the results, and then passes the sum through an activation function. The activation function is used to decide whether the perceptron should be activated or not. If the perceptron is activated, it outputs 1, otherwise, it outputs 0.\n",
        "\n",
        "Perceptron Value = (Input1 * Weight1) + (Input2 * Weight2) + (Input3 * Weight3) + ... + (InputN * WeightN)\n",
        "\n",
        "Perceptron Output = Activation Function(Perceptron Value)\n",
        "\n",
        "Activation Function = Could by any function that takes a number and returns 1 or 0 (for binary classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22yWjjiagdT5"
      },
      "source": [
        "## Perceptron Learning Algorithm (Weight and Bias Update)\n",
        "\n",
        "The perceptron learning algorithm is used to update the weights and bias of a perceptron. The algorithm is as follows:\n",
        "\n",
        "1. Initialize the weights to random values (or zero) and the bias to zero.\n",
        "2. For each training example (x, y):\n",
        "    1. Calculate the output value (y_hat) using the perceptron equation (y_hat = activation_function((x1 * w1) + (x2 * w2) + ... + (xn * wn) + b))\n",
        "    2. Update the weights and bias using the perceptron learning rule (w = w + (learning_rate * (y - y_hat) * x) and b = b + (learning_rate * (y - y_hat)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZGMFGK1gdT6"
      },
      "source": [
        "## Implementing a simple Single Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjG2_S2fgdT6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def unit_step_func(x):\n",
        "    return np.where(x > 0 , 1, 0)\n",
        "\n",
        "class Perceptron:\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # init parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        y_ = np.where(y > 0 , 1, 0)\n",
        "\n",
        "        # learn weights\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_func(linear_output)\n",
        "\n",
        "                # Perceptron update rule\n",
        "                update = self.lr * (y_[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_func(linear_output)\n",
        "        return y_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYuYpkJUgdT7"
      },
      "source": [
        "## Getting Prediction using our above perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJkqLzCAgdT8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = datasets.make_blobs(\n",
        "        n_samples=5000, n_features=2, centers=2, cluster_std=2.05, random_state=20\n",
        "    )\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=20\n",
        ")\n",
        "\n",
        "# Using perceptron\n",
        "p = Perceptron(learning_rate=0.01, n_iters=100)\n",
        "\n",
        "p.fit(X_train, y_train)\n",
        "\n",
        "predictions = p.predict(X_test)\n",
        "\n",
        "print(\"Perceptron classification accuracy\", accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "vltgH2mVvnpK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "X, y = np.random.rand(1000, 5), np.random.choice([0, 1], size=1000)"
      ],
      "metadata": {
        "id": "OsUfLa77v6q5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2rFFlwGXv60E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation_functions = ['identity','logistic', 'tanh', 'relu']\n",
        "accuracies = []"
      ],
      "metadata": {
        "id": "Ff6h9Msov64-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for activation in activation_functions:\n",
        "    if activation == 'identity':\n",
        "        model = MLPClassifier(random_state=42, max_iter=1000, activation=activation)\n",
        "    else:\n",
        "        model = MLPClassifier(hidden_layer_sizes=(100,), activation=activation, random_state=42, max_iter=1000)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    accuracies.append((activation, accuracy))"
      ],
      "metadata": {
        "id": "IcX6TIFEv68v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for activation, accuracy in accuracies:\n",
        "    print(f'{activation.capitalize()} Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dMZ19VBv7E_",
        "outputId": "dcde6909-ffa0-4e87-dddd-21212309a3b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identity Accuracy: 0.5450\n",
            "Logistic Accuracy: 0.5050\n",
            "Tanh Accuracy: 0.5450\n",
            "Relu Accuracy: 0.5100\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI_Lab-lMum5T4J",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}